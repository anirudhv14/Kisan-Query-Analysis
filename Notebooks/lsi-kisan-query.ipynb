{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4de0b519-dd3d-4aa3-bd68-d047af1216e1",
    "_uuid": "31418100-226e-4e49-bc1a-4b1b09baceda",
    "execution": {
     "iopub.execute_input": "2021-11-20T09:04:32.852117Z",
     "iopub.status.busy": "2021-11-20T09:04:32.851751Z",
     "iopub.status.idle": "2021-11-20T09:05:19.370378Z",
     "shell.execute_reply": "2021-11-20T09:05:19.369479Z",
     "shell.execute_reply.started": "2021-11-20T09:04:32.851999Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:06:43.949879Z",
     "iopub.status.busy": "2021-11-20T09:06:43.949582Z",
     "iopub.status.idle": "2021-11-20T09:06:46.101695Z",
     "shell.execute_reply": "2021-11-20T09:06:46.101100Z",
     "shell.execute_reply.started": "2021-11-20T09:06:43.949844Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:08:12.252849Z",
     "iopub.status.busy": "2021-11-20T09:08:12.252583Z",
     "iopub.status.idle": "2021-11-20T09:08:12.265281Z",
     "shell.execute_reply": "2021-11-20T09:08:12.264399Z",
     "shell.execute_reply.started": "2021-11-20T09:08:12.252820Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in tqdm(enumerate(ldamodel[corpus])):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:08:14.969600Z",
     "iopub.status.busy": "2021-11-20T09:08:14.968982Z",
     "iopub.status.idle": "2021-11-20T09:08:40.439941Z",
     "shell.execute_reply": "2021-11-20T09:08:40.439405Z",
     "shell.execute_reply.started": "2021-11-20T09:08:14.969562Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/kisan-query-analysis-dataset/query_agg.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:09:39.212509Z",
     "iopub.status.busy": "2021-11-20T09:09:39.211767Z",
     "iopub.status.idle": "2021-11-20T09:09:40.516995Z",
     "shell.execute_reply": "2021-11-20T09:09:40.516257Z",
     "shell.execute_reply.started": "2021-11-20T09:09:39.212466Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['QueryText'].notna()]\n",
    "data['KccAns'].fillna(\"Nan\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:09:42.965557Z",
     "iopub.status.busy": "2021-11-20T09:09:42.965269Z",
     "iopub.status.idle": "2021-11-20T09:09:42.970111Z",
     "shell.execute_reply": "2021-11-20T09:09:42.968926Z",
     "shell.execute_reply.started": "2021-11-20T09:09:42.965528Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:20:51.942878Z",
     "iopub.status.busy": "2021-11-20T09:20:51.942143Z",
     "iopub.status.idle": "2021-11-20T09:20:52.509380Z",
     "shell.execute_reply": "2021-11-20T09:20:52.508632Z",
     "shell.execute_reply.started": "2021-11-20T09:20:51.942837Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_qt = pickle.load(open(\"../input/kisantfidf/dictionary\", \"rb\"))\n",
    "tfidf_qt = pickle.load(open(\"../input/kisan-tfidf/tfidf\", \"rb\"))\n",
    "#lda_model_qt_tfidf = pickle.load(open(\"models\\\\lda_model_tfidf\", \"rb\"))\n",
    "# corpus_qt_tfidf = pickle.load(open(\"models\\\\corpus_tfidf\", \"rb\"))\n",
    "corp = pickle.load(open(\"../input/kisan-tfidf/corp_tfidf\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:22:21.929294Z",
     "iopub.status.busy": "2021-11-20T09:22:21.928552Z",
     "iopub.status.idle": "2021-11-20T09:22:26.087558Z",
     "shell.execute_reply": "2021-11-20T09:22:26.086605Z",
     "shell.execute_reply.started": "2021-11-20T09:22:21.929251Z"
    }
   },
   "outputs": [],
   "source": [
    "lsi_model_qt_tfidf = gensim.models.LsiModel(corp, num_topics=8, id2word=dictionary_qt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:22:55.849303Z",
     "iopub.status.busy": "2021-11-20T09:22:55.848992Z",
     "iopub.status.idle": "2021-11-20T09:22:55.859522Z",
     "shell.execute_reply": "2021-11-20T09:22:55.858892Z",
     "shell.execute_reply.started": "2021-11-20T09:22:55.849267Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, topic in lsi_model_qt_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:28:25.623464Z",
     "iopub.status.busy": "2021-11-20T09:28:25.623151Z",
     "iopub.status.idle": "2021-11-20T09:28:25.632849Z",
     "shell.execute_reply": "2021-11-20T09:28:25.632269Z",
     "shell.execute_reply.started": "2021-11-20T09:28:25.623435Z"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(tt):\n",
    "    tt = pd.Series(tt)\n",
    "    processed = tt.map(preprocess)\n",
    "#     dictionary = gensim.corpora.Dictionary(processed)\n",
    "    bow_corpus = [dictionary_qt.doc2bow(doc) for doc in processed]\n",
    "#     tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf_qt[bow_corpus]\n",
    "#     print(\"Processed: {} \\n BoW: {} \\n Corpus TF-IDF: {}\".format(processed, bow_corpus, corpus_tfidf[0]))\n",
    "#     ctr = True\n",
    "    for index, score in sorted(lsi_model_qt_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lsi_model_qt_tfidf.print_topic(index, 8)))\n",
    "    \n",
    "    tops = lsi_model_qt_tfidf.get_document_topics(bow_corpus[0])\n",
    "    max1 = (0,0)\n",
    "    for i in tops:\n",
    "        if(i[1] > max1[1]):\n",
    "            max1 = i\n",
    "    aa1 = df_dominant_topic[df_dominant_topic[\"Text\"].notna()]\n",
    "    pos_doc = aa1[aa1[\"Dominant_Topic\"] == max1[0]][:10]\n",
    "    a1 = pos_doc[\"Text\"].tolist()\n",
    "#     fin_ans = dd[dd['QueryText'] == a1]['KccAns']\n",
    "    \n",
    "    fin_ans = []\n",
    "    for i in data.values:\n",
    "        if(i[5] in a1):\n",
    "            fin_ans.append(i[6])\n",
    "    return fin_ans[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T09:28:25.877617Z",
     "iopub.status.busy": "2021-11-20T09:28:25.877351Z",
     "iopub.status.idle": "2021-11-20T09:28:25.910976Z",
     "shell.execute_reply": "2021-11-20T09:28:25.909940Z",
     "shell.execute_reply.started": "2021-11-20T09:28:25.877581Z"
    }
   },
   "outputs": [],
   "source": [
    "aa = \"rice\"\n",
    "fin = pipeline(aa)\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
